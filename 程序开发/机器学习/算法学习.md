# Logistic regression

*分类算法，有监督算法*

线性回归拿来做分类会使得远大于1或者远小于0的特征值对loss函数产生巨大的影响，使得决策边界出现偏差，所以产生了logistic regression 主要目的是限制输出变量在0~1之间，在原先的函数上添加sigmoid激励函数

$$ h_{\theta}(x)=Sigmoid(\theta^Tx) $$

## 激励函数sigmoid function
  $$g(z)= \frac{1}{1+e^{-z}} $$

将输出向量输出在0~1之间，输出的是达成条件的概率

## 非线性问题

当出现非线性问题时候（eg：决策边界是个圆形）在原来的线性特征向量加上二次项

$$ \left[ 
    \begin{matrix}
    x_0\\
    x_1\\
    \end{matrix}
    \right]
    
    \to

    \left[
     \begin{matrix}
     x_0\\
     x_1\\
     x_0^2\\
     x_1^2\\
     \end{matrix}   
        \right]
    
     $$

添加了两个特征量，再加入两个$\theta$量就可以了

## lossfunction

cost函数为(corss entropy：

$$ Cost(h_\theta(x), y) = -y \log(h_\theta(x))-(1-y)\log(1-h_\theta(x))
$$

整个函数的loss junction 定义为：

$$ J(\theta) = \frac{1}{m}\sum^m_{i=1}Cost(h_\theta(x^i),y^i)
$$

> ### loss function的推导：
> 使用的是条件概率的模型
> $$ p(y=1|x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}$$
>$$ p(y=o|x) = \frac{1}{1+e^{w^Tx+b}}$$
>通过这两个公式进行极大似然求解：
>简历极大似然模型：
>$$l(w,b)=\sum^m_{i=1}\ln p(y_i|x_i;w,b)$$
>使得其中属于真是标记的概率越大越好：
>$$p(y_i|xi;w,b)=p(y=0|x_i;w,b)^{(1-yo)}*p(y=1|x_i;w,b)^y_i$$
>带入极大似然后经过ln函数求解ok了

## 优化方法

- ### 高级优化算法：
  
  **优点**：
  - 不用手动挑选学习速率
  - 收敛比梯度下降更快
  
  **缺点**：
  - 非常复杂

- ### Gradient 下降

  参数优化和logistic regression一致

$$ w_i\leftarrow w_i-\eta\sum_n-(\hat{y}^n-f_{w,b}(x^n))x_i^n$$
 
## logistic regression和linear regression 的区别

||logistic regression|Linear Regression|
|---|---|---|
|表达式|$f_{w,b}(x)=\sigma(\sum_iw_ix_i+b)$|$f_{w,b}(x)=\sum_iw_ix_i+b$|
|loss function|$J(\theta) = \frac{1}{m}\sum^m_{i=1}Cost(h_\theta(x^i),y^i)$|$J(\theta)=\frac{1}{2}\sum_i(h_\theta(x^i)-y^i)^2$|
|优化函数|都是：$w_i\leftarrow w_i-\eta\sum_n-(\hat{y}^n-f_{w,b}(x^n))x_i^n$|

## logistic regression的算法特点

- 优点： 计算代价不高，已于理解和实现
- 缺点： 容易欠拟合，分类精度可能不高
- 适用的数据类型：数值型和标称数据

# SVM

- 支持向量机就是隔离分隔超平面最近的那些点
- 机就是表示一种算法，而不是表示机器

## 寻找最大间隔

 

